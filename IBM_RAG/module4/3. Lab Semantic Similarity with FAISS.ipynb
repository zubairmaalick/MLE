{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity with FAISS\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "Welcome to a hands-on exploration of semantic search, where we unravel the intricacies of finding meaning in text. This lab is a beginner's journey into the realm of advanced information retrieval. You'll start by learning the essentials of text preprocessing to enhance data quality. Next, you'll dive into the world of vector spaces, using the Universal Sentence Encoder to convert text into a format that machines understand. Finally, you'll harness the efficiency of FAISS, a library built for rapid similarity search, to compare and retrieve information. By the end of our session, you'll have a functional semantic search engine that not only understands the subtleties of human language but also fetches information that truly matters.\n",
    "\n",
    "<p style='color: red'>Embark on this learning adventure to build a search engine that sees beyond the obvious, leveraging context and semantics to satisfy the quest for information.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Understanding-Semantic-Search\">Understanding Semantic Search</a>\n",
    "    </li>\n",
    "    <li><a href=\"#Understanding-Vectorization-and-Indexing\">Understanding Vectorization and Indexing</a></li>\n",
    "    <li><a href=\"#The-20-Newsgroups-Dataset\">The 20 Newsgroups Dataset</a></li>\n",
    "    <li><a href=\"#Pre-processing-Data\">Pre-processing Data</a></li>\n",
    "    <li><a href=\"#Universal-Sentence-Encoder\">Universal Sentence Encoder</a></li>\n",
    "    <li><a href=\"#Indexing-with-FAISS\">Indexing with FAISS</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "In this lab, our objectives are to:\n",
    "\n",
    "- Understand the fundamentals of semantic search and its advantages over traditional search methods.\n",
    "- Familiarize with the process of preparing text data for semantic analysis, including cleaning and standardization techniques.\n",
    "- Learn how to utilize the Universal Sentence Encoder to convert text into high-dimensional vector space representations.\n",
    "- Gain practical experience with FAISS (Facebook AI Similarity Search), an efficient library for indexing and searching high-dimensional vectors.\n",
    "- Apply these techniques to build a fully functioning semantic search engine that can interpret and respond to natural language queries.\n",
    "\n",
    "By accomplishing these objectives, you will acquire a comprehensive skill set that underpins advanced search functionalities in modern AI-driven systems, preparing you for further exploration and development in the field of natural language processing and information retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To ensure a smooth experience throughout this lab, we need to set up our environment properly. This includes installing necessary libraries, importing them, and preparing helper functions that will be used later in the lab.\n",
    "\n",
    "## Installing Required Libraries\n",
    "\n",
    "Before we start, you need to install the following libraries if you haven't already:\n",
    "\n",
    "- `tensorflow`: The core library for TensorFlow, required for working with the Universal Sentence Encoder.\n",
    "- `tensorflow-hub`: A library that makes it easy to download and deploy pre-trained TensorFlow models, including the Universal Sentence Encoder.\n",
    "- `faiss-cpu`: A library for efficient similarity search and clustering of dense vectors.\n",
    "- `numpy`: A library for numerical computing, which we will use to handle arrays and matrices.\n",
    "- `scikit-learn`: A machine learning library that provides various tools for data mining and data analysis, useful for additional tasks like data splitting and evaluation metrics.\n",
    "\n",
    "You can install these libraries using `pip` with the following commands:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (0.20.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Collecting tensorflow>=2.0.0\n",
      "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m362.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (1.4.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.0.0)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow>=2.0.0)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow>=2.0.0)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (1.48.1)\n",
      "Collecting h5py>=2.9.0 (from tensorflow>=2.0.0)\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.12,>=2.11.0 (from tensorflow>=2.0.0)\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow>=2.0.0)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (1.21.6)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow>=2.0.0)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (23.1)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow>=2.0.0)\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (1.16.0)\n",
      "Collecting tensorboard<2.12,>=2.11 (from tensorflow>=2.0.0)\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow>=2.0.0)\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow>=2.0.0) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=2.0.0)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0) (0.40.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading google_auth-2.41.1-py2.py3-none-any.whl (221 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.3/221.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (2.29.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (2.2.3)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (0.3.0)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0.0) (0.5.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0.0)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, h5py, gast, cachetools, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 1.14.0\n",
      "    Uninstalling tensorflow-estimator-1.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.8\n",
      "    Uninstalling protobuf-4.21.8:\n",
      "      Successfully uninstalled protobuf-4.21.8\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.1.6\n",
      "    Uninstalling Keras-2.1.6:\n",
      "      Successfully uninstalled Keras-2.1.6\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.8.0\n",
      "    Uninstalling h5py-2.8.0:\n",
      "      Successfully uninstalled h5py-2.8.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.3\n",
      "    Uninstalling gast-0.5.3:\n",
      "      Successfully uninstalled gast-0.5.3\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.14.0\n",
      "    Uninstalling tensorboard-1.14.0:\n",
      "      Successfully uninstalled tensorboard-1.14.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 1.14.0\n",
      "    Uninstalling tensorflow-1.14.0:\n",
      "      Successfully uninstalled tensorflow-1.14.0\n",
      "Successfully installed astunparse-1.6.3 cachetools-5.5.2 flatbuffers-25.9.23 gast-0.4.0 google-auth-2.41.1 google-auth-oauthlib-0.4.6 h5py-3.8.0 keras-2.11.0 libclang-18.1.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.34.0\n",
      "Collecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow-hub) (1.21.6)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow-hub) (3.19.6)\n",
      "INFO: pip is looking at multiple versions of tensorflow-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensorflow_hub-0.16.0-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu numpy scikit-learn\n",
    "!pip install \"tensorflow>=2.0.0\"\n",
    "!pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 12:19:07.730316: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-07 12:19:07.959633: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-07 12:19:07.964642: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-10-07 12:19:07.964684: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-10-07 12:19:08.765784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-10-07 12:19:08.765967: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-10-07 12:19:08.765986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import faiss\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# Suppressing warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Semantic Search\n",
    "\n",
    "When we're looking to build a semantic search engine, it's important to start with the basics. Let's break down what semantic search is and why it's a game-changer in finding information.\n",
    "\n",
    "### What is Semantic Search?\n",
    "\n",
    "Semantic search transcends the limitations of traditional keyword searches by understanding the context and nuances of language in user queries. At its core, semantic search:\n",
    "\n",
    "- Enhances the search experience by interpreting the intent and contextual meaning behind search queries.\n",
    "- Delivers more accurate and relevant search results by analyzing the relationships between words and phrases within the search context.\n",
    "- Adapts to user behavior and preferences, refining search results for better user satisfaction.\n",
    "\n",
    "### How Semantic Search Works - The Simple Version\n",
    "\n",
    "Now, how does this smart assistant do its job? It uses some clever tricks from a field called Natural Language Processing, or NLP for short. Here’s the simple version of the process:\n",
    "\n",
    "- **Getting the Gist**: First up, the search engine listens to your query and tries to get the gist of it. Instead of just spotting keywords, it digs deeper to find the real meaning.\n",
    "- **Making Connections**: Next, it thinks about all the different ways words can be related (like \"doctor\" and \"physician\" meaning the same thing). This helps it get a better sense of what you're asking for.\n",
    "- **Picking the Best**: Finally, it acts like a librarian who knows every book in the library. It sorts through tons of information to pick what matches your query best, considering what you probably mean.\n",
    "\n",
    "### The Technical Side of Semantic Search\n",
    "\n",
    "After understanding the basics, let's peek under the hood at the technical engine powering semantic search. This part is a bit like math class, where we learn about vectors — no, not the ones you learned in physics, but something similar that we use in search engines.\n",
    "\n",
    "#### Vectors: The Language of Semantic Search\n",
    "\n",
    "In the world of semantic search, a vector is a list of numbers that a computer uses to represent the meaning of words or sentences. Imagine each word or sentence as a point in space. The closer two points are, the more similar their meanings.\n",
    "\n",
    "- **Creating Vectors**: We start by turning words or sentences into vectors using models like the Universal Sentence Encoder. It's like giving each piece of text its unique numerical fingerprint.\n",
    "- **Calculating Similarity**: To find out how similar two pieces of text are, we measure how close their vectors are in space. This is done using mathematical formulas, such as cosine similarity, which tells us how similar or different two text fingerprints are.\n",
    "- **Using Vectors for Search**: When you search for something, the search engine looks for the vectors closest to the vector of your query. The closest vectors represent the most relevant results to what you're asking.\n",
    "\n",
    "#### How Vectors Power Our Search\n",
    "\n",
    "Vectors are powerful because they can capture the subtle meanings of language that go beyond the surface of words. Here's what happens in a semantic search engine:\n",
    "\n",
    "1. **Vectorization**: When we type in a search query, the engine immediately turns our words into a vector.\n",
    "2. **Indexing**: It then quickly scans through a massive index of other vectors, each representing different pieces of information.\n",
    "3. **Retrieval**: By finding the closest matching vectors, the engine retrieves information that's not just textually similar but semantically related.\n",
    "\n",
    "By the end of this guide, you'll understand how to create a search engine that does all of this and more. We'll start simple and build up step by step. Ready? Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Vectorization and Indexing\n",
    "\n",
    "Vectorization and indexing are key components of building a semantic search engine. Let's explore how they work using the Universal Sentence Encoder (USE) and FAISS.\n",
    "\n",
    "### What does the Universal Sentence Encoder do?\n",
    "\n",
    "The Universal Sentence Encoder (USE) takes sentences, no matter how complex, and turns them into vectors. These vectors are arrays of numbers that capture the essence of sentences. Here's why it's amazing:\n",
    "\n",
    "- **Language Comprehension**: USE understands the meaning of sentences by considering the context in which each word is used.\n",
    "- **Versatility**: It's trained on a variety of data sources, enabling it to handle a wide range of topics and sentence structures.\n",
    "- **Speed**: Once trained, USE can quickly convert sentences to vectors, making it highly efficient.\n",
    "\n",
    "### How does the Universal Sentence Encoder work?\n",
    "\n",
    "The magic of USE lies in its training. It uses deep learning models to digest vast amounts of text. Here’s what it does:\n",
    "\n",
    "1. **Analyzes Words**: It looks at each word in a sentence and the words around it to get a full picture of their meaning.\n",
    "2. **Understands Context**: It pays attention to the order of words and how they're used together to grasp the sentence's intent.\n",
    "3. **Creates Vectors**: It converts all this understanding into a numeric vector that represents the sentence.\n",
    "\n",
    "### What is FAISS and what does it do?\n",
    "\n",
    "FAISS, developed by Facebook AI, is a library for efficient similarity search. After we have vectors from USE, we need a way to search through them quickly to find the most relevant ones to a query. FAISS does just that:\n",
    "\n",
    "- **Efficient Searching**: It uses optimized algorithms to rapidly search through large collections of vectors.\n",
    "- **Scalability**: It can handle databases of vectors that are too large to fit in memory, making it suitable for big data applications.\n",
    "- **Accuracy**: It provides highly accurate search results, thanks to its advanced indexing strategies.\n",
    "\n",
    "### How does FAISS work?\n",
    "\n",
    "FAISS creates an index of all the vectors, which allows it to search through them efficiently. Here's a simplified version of its process:\n",
    "\n",
    "1. **Index Building**: It organizes vectors in a way that similar ones are near each other, making it faster to find matches.\n",
    "2. **Searching**: When you search with a new vector, FAISS quickly identifies which part of the index to look at for the closest matches.\n",
    "3. **Retrieving Results**: It then retrieves the most similar vectors, which correspond to the most relevant search results.\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "With USE and FAISS, we have a powerful duo. USE helps us understand language in numerical terms, and FAISS lets us search through these numbers to find meaningful connections. Combining them, we create a semantic search engine that's both smart and swift.\n",
    "\n",
    "<!-- Insert a diagram that visually represents the flow from text input to vectorization with USE to searching and indexing with FAISS -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 20 Newsgroups Dataset\n",
    "\n",
    "In this project, we'll be using the 20 Newsgroups dataset, a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. It's a go-to dataset in the NLP community because it presents real-world challenges:\n",
    "\n",
    "### What is the 20 Newsgroups Dataset?\n",
    "\n",
    "- **Diverse Topics**: The dataset spans 20 different topics, from sports and science to politics and religion, reflecting the diverse interests of newsgroup members.\n",
    "- **Natural Language**: It contains actual discussions, with all the nuances of human language, making it ideal for semantic search.\n",
    "- **Prevalence of Context**: The conversations within it require understanding of context to differentiate between the topics effectively.\n",
    "\n",
    "### How are we using the 20 Newsgroups Dataset?\n",
    "\n",
    "1. **Exploring Data**: We'll start by loading the dataset and exploring its structure to understand the kind of information it holds.\n",
    "2. **Preprocessing**: We'll clean the text data, removing any unwanted noise that could affect our semantic analysis.\n",
    "3. **Vectorization**: We'll then use the Universal Sentence Encoder to transform this text into numerical vectors that capture the essence of each document.\n",
    "4. **Semantic Search Implementation**: Finally, we'll use FAISS to index these vectors, allowing us to perform fast and efficient semantic searches across the dataset.\n",
    "\n",
    "By working with the 20 Newsgroups dataset, you'll gain hands-on experience with real-world data and the end-to-end process of building a semantic search engine.\n",
    "\n",
    "<!-- An image of a sample newsgroup post or a chart showing the distribution of topics within the dataset can be helpful here -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample post 1:\n",
      "\n",
      "(\"From: lerxst@wam.umd.edu (where's my thing)\\n\"\n",
      " 'Subject: WHAT car is this!?\\n'\n",
      " 'Nntp-Posting-Host: rac3.wam.umd.edu\\n'\n",
      " 'Organization: University of Maryland, College Park\\n'\n",
      " 'Lines: 15\\n'\n",
      " '\\n'\n",
      " ' I was wondering if anyone out there could enlighten me on this car I saw\\n'\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/\\n'\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition,\\n'\n",
      " 'the front bumper was separate from the rest of the body. This is \\n'\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years\\n'\n",
      " 'of production, where this car is made, history, or whatever info you\\n'\n",
      " 'have on this funky looking car, please e-mail.\\n'\n",
      " '\\n'\n",
      " 'Thanks,\\n'\n",
      " '- IL\\n'\n",
      " '   ---- brought to you by your neighborhood Lerxst ----\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample post 2:\n",
      "\n",
      "('From: guykuo@carson.u.washington.edu (Guy Kuo)\\n'\n",
      " 'Subject: SI Clock Poll - Final Call\\n'\n",
      " 'Summary: Final call for SI clock reports\\n'\n",
      " 'Keywords: SI,acceleration,clock,upgrade\\n'\n",
      " 'Article-I.D.: shelley.1qvfo9INNc3s\\n'\n",
      " 'Organization: University of Washington\\n'\n",
      " 'Lines: 11\\n'\n",
      " 'NNTP-Posting-Host: carson.u.washington.edu\\n'\n",
      " '\\n'\n",
      " 'A fair number of brave souls who upgraded their SI clock oscillator have\\n'\n",
      " 'shared their experiences for this poll. Please send a brief message '\n",
      " 'detailing\\n'\n",
      " 'your experiences with the procedure. Top speed attained, CPU rated speed,\\n'\n",
      " 'add on cards and adapters, heat sinks, hour of usage per day, floppy disk\\n'\n",
      " 'functionality with 800 and 1.4 m floppies are especially requested.\\n'\n",
      " '\\n'\n",
      " 'I will be summarizing in the next two days, so please add to the network\\n'\n",
      " \"knowledge base if you have done the clock upgrade and haven't answered this\\n\"\n",
      " 'poll. Thanks.\\n'\n",
      " '\\n'\n",
      " 'Guy Kuo <guykuo@u.washington.edu>\\n')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample post 3:\n",
      "\n",
      "('From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\n'\n",
      " 'Subject: PB questions...\\n'\n",
      " 'Organization: Purdue University Engineering Computer Network\\n'\n",
      " 'Distribution: usa\\n'\n",
      " 'Lines: 36\\n'\n",
      " '\\n'\n",
      " 'well folks, my mac plus finally gave up the ghost this weekend after\\n'\n",
      " \"starting life as a 512k way back in 1985.  sooo, i'm in the market for a\\n\"\n",
      " 'new machine a bit sooner than i intended to be...\\n'\n",
      " '\\n'\n",
      " \"i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\\n\"\n",
      " 'of questions that (hopefully) somebody can answer:\\n'\n",
      " '\\n'\n",
      " '* does anybody know any dirt on when the next round of powerbook\\n'\n",
      " \"introductions are expected?  i'd heard the 185c was supposed to make an\\n\"\n",
      " 'appearence \"this summer\" but haven\\'t heard anymore on it - and since i\\n'\n",
      " \"don't have access to macleak, i was wondering if anybody out there had\\n\"\n",
      " 'more info...\\n'\n",
      " '\\n'\n",
      " '* has anybody heard rumors about price drops to the powerbook line like the\\n'\n",
      " \"ones the duo's just went through recently?\\n\"\n",
      " '\\n'\n",
      " \"* what's the impression of the display on the 180?  i could probably swing\\n\"\n",
      " \"a 180 if i got the 80Mb disk rather than the 120, but i don't really have\\n\"\n",
      " 'a feel for how much \"better\" the display is (yea, it looks great in the\\n'\n",
      " 'store, but is that all \"wow\" or is it really that good?).  could i solicit\\n'\n",
      " 'some opinions of people who use the 160 and 180 day-to-day on if its worth\\n'\n",
      " 'taking the disk size and money hit to get the active display?  (i realize\\n'\n",
      " \"this is a real subjective question, but i've only played around with the\\n\"\n",
      " 'machines in a computer store breifly and figured the opinions of somebody\\n'\n",
      " 'who actually uses the machine daily might prove helpful).\\n'\n",
      " '\\n'\n",
      " '* how well does hellcats perform?  ;)\\n'\n",
      " '\\n'\n",
      " \"thanks a bunch in advance for any info - if you could email, i'll post a\\n\"\n",
      " 'summary (news reading time is at a premium with finals just around the\\n'\n",
      " 'corner... :( )\\n'\n",
      " '--\\n'\n",
      " 'Tom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical '\n",
      " 'Engineering\\n'\n",
      " '---------------------------------------------------------------------------\\n'\n",
      " '\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\n'\n",
      " 'Nietzsche\\n')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the first 3 posts from the dataset\n",
    "for i in range(3):\n",
    "    print(f\"Sample post {i+1}:\\n\")\n",
    "    pprint(newsgroups_train.data[i])\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Data\n",
    "\n",
    "In this section, we focus on preparing the text data from the 20 Newsgroups dataset for our semantic search engine. Preprocessing is a critical step to ensure the quality and consistency of the data before it's fed into the Universal Sentence Encoder.\n",
    "\n",
    "## Steps in Preprocessing:\n",
    "\n",
    "1. **Fetching Data**: \n",
    "   - We load the complete 20 Newsgroups dataset using `fetch_20newsgroups` from `sklearn.datasets`. \n",
    "   - `documents = newsgroups.data` stores all the newsgroup documents in a list.\n",
    "\n",
    "2. **Defining the Preprocessing Function**:\n",
    "   - The `preprocess_text` function is designed to clean each text document. Here's what it does to every piece of text:\n",
    "     - **Removes Email Headers**: Strips off lines that start with 'From:' as they usually contain metadata like email addresses.\n",
    "     - **Eliminates Email Addresses**: Finds patterns resembling email addresses and removes them.\n",
    "     - **Strips Punctuations and Numbers**: Removes all characters except alphabets, aiding in focusing on textual data.\n",
    "     - **Converts to Lowercase**: Standardizes the text by converting all characters to lowercase, ensuring uniformity.\n",
    "     - **Trims Excess Whitespace**: Cleans up any extra spaces, tabs, or line breaks.\n",
    "\n",
    "3. **Applying Preprocessing**:\n",
    "   - We iterate over each document in the `documents` list and apply our `preprocess_text` function.\n",
    "   - The cleaned documents are stored in `processed_documents`, ready for further processing.\n",
    "\n",
    "By preprocessing the text data in this way, we reduce noise and standardize the text, which is essential for achieving meaningful semantic analysis in later steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "documents = newsgroups.data\n",
    "\n",
    "# Basic preprocessing of text data\n",
    "def preprocess_text(text):\n",
    "    # Remove email headers\n",
    "    text = re.sub(r'^From:.*\\n?', '', text, flags=re.MULTILINE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess each document\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original post:\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Preprocessed post:\n",
      "\n",
      "subject what car is this nntppostinghost racwamumdedu organization university of maryland college park lines i was wondering if anyone out there could enlighten me on this car i saw the other day it was a door sports car looked to be from the late s early s it was called a bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all i know if anyone can tellme a model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please email thanks il brought to you by your neighborhood lerxst\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose a sample post to display\n",
    "sample_index = 0  # for example, the first post in the dataset\n",
    "\n",
    "# Print the original post\n",
    "print(\"Original post:\\n\")\n",
    "print(newsgroups_train.data[sample_index])\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the preprocessed post\n",
    "print(\"Preprocessed post:\\n\")\n",
    "print(preprocess_text(newsgroups_train.data[sample_index]))\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder\n",
    "\n",
    "After preprocessing the text data, the next step is to transform this cleaned text into numerical vectors using the Universal Sentence Encoder (USE). These vectors capture the semantic essence of the text.\n",
    "\n",
    "### Loading the USE Module:\n",
    "\n",
    "- We use TensorFlow Hub (`hub`) to load the pre-trained Universal Sentence Encoder.\n",
    "- `embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")` fetches the USE module, making it ready for vectorization.\n",
    "\n",
    "### Defining the Embedding Function:\n",
    "\n",
    "- The `embed_text` function is defined to take a piece of text as input and return its vector representation.\n",
    "- Inside the function, `embed(text)` converts the text into a high-dimensional vector, capturing the nuanced semantic meaning.\n",
    "- `.numpy()` is used to convert the result from a TensorFlow tensor to a NumPy array, which is a more versatile format for subsequent operations.\n",
    "\n",
    "### Vectorizing Preprocessed Documents:\n",
    "\n",
    "- We then apply the `embed_text` function to each document in our preprocessed dataset, `processed_documents`.\n",
    "- `np.vstack([...])` stacks the vectors vertically to create a 2D array, where each row represents a document.\n",
    "- The resulting array `X_use` holds the vectorized representations of all the preprocessed documents, ready to be used for semantic search indexing and querying.\n",
    "\n",
    "By vectorizing the text with USE, we've now converted our textual data into a format that can be efficiently processed by machine learning algorithms, setting the stage for the next step: indexing with FAISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 12:22:37.059255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-10-07 12:22:37.059324: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-10-07 12:22:37.059353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterlab-zubairmaalic): /proc/driver/nvidia/version does not exist\n",
      "2025-10-07 12:22:37.059791: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Function to generate embeddings\n",
    "def embed_text(text):\n",
    "    return embed(text).numpy()\n",
    "\n",
    "# Generate embeddings for each preprocessed document\n",
    "X_use = np.vstack([embed_text([doc]) for doc in processed_documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing with FAISS\n",
    "\n",
    "With our documents now represented as vectors using the Universal Sentence Encoder, the next step is to use FAISS (Facebook AI Similarity Search) for efficient similarity searching.\n",
    "\n",
    "## Creating a FAISS Index:\n",
    "\n",
    "- We first determine the dimension of our vectors from `X_use` using `X_use.shape[1]`.\n",
    "- A FAISS index (`index`) is created specifically for L2 distance (Euclidean distance) using `faiss.IndexFlatL2(dimension)`.\n",
    "- We add our document vectors to this index with `index.add(X_use)`. This step effectively creates a searchable space for our document vectors.\n",
    "\n",
    "### Choosing the Right Index:\n",
    "\n",
    "- In this project, we use `IndexFlatL2` for its simplicity and effectiveness in handling small to medium-sized datasets.\n",
    "- FAISS offers a variety of indexes tailored for different use cases and dataset sizes. Depending on your specific needs and the complexity of your data, you might consider other indexes for more efficient searching.\n",
    "- For larger datasets or more advanced use cases, indexes like `IndexIVFFlat`, `IndexIVFPQ`, and others can provide faster search times and reduced memory usage. Explore more at [FAISS indexes wiki](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dimension = X_use.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # Creating a FAISS index\n",
    "index.add(X_use)  # Adding the document vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Quering with FAISS\n",
    "### Defining the Search Function:\n",
    "\n",
    "- The `search` function is designed to find documents that are semantically similar to a given query.\n",
    "- It preprocesses the query text using the `preprocess_text` function to ensure consistency.\n",
    "- The query text is then converted to a vector using `embed_text`.\n",
    "- FAISS performs a search for the nearest neighbors (`k`) to this query vector in our index.\n",
    "- It returns the distances and indices of these nearest neighbors.\n",
    "\n",
    "### Executing a Query and Displaying Results:\n",
    "\n",
    "- We test our search engine with an example query (e.g., \"motorcycle\").\n",
    "- The `search` function returns the indices of the documents in the index that are most similar to the query.\n",
    "- For each result, we display:\n",
    "   - The ranking of the result (based on distance).\n",
    "   - The distance value itself, indicating how close the document is to the query.\n",
    "   - The actual text of the document. We display both the preprocessed and original versions of each document for comparison.\n",
    "\n",
    "This functionality showcases the practical application of semantic search: retrieving information that is contextually relevant to the query, not just based on keyword matching. The displayed results will give a clear idea of how our semantic search engine interprets and responds to natural language queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a query using the Faiss index\n",
    "def search(query_text, k=5):\n",
    "    # Preprocess the query text\n",
    "    preprocessed_query = preprocess_text(query_text)\n",
    "    # Generate the query vector\n",
    "    query_vector = embed_text([preprocessed_query])\n",
    "    # Perform the search\n",
    "    distances, indices = index.search(query_vector.astype('float32'), k)\n",
    "    return distances, indices\n",
    "\n",
    "# Example Query\n",
    "query_text = \"motorcycle\"\n",
    "distances, indices = search(query_text)\n",
    "\n",
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Ensure that the displayed document is the preprocessed one\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{processed_documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Displaying the original (unprocessed) document corresponding to the search result\n",
    "    print(f\"Rank {i+1}: (Distance: {distances[0][i]})\\n{documents[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "<details>\n",
    "    <summary>Click here for the changelog</summary>\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-01-08|0.1|Ashutosh Sagar|SME initial creation|\n",
    "|2025-07-17|0.2|Steve Ryan|ID review and format fixes|\n",
    "|2025-07-25|0.3|Steve Ryan|ID fixed TOC and lab title|\n",
    "\n",
    "</detials>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "faf67be6dc53caed3c38c6e488c2b44148930d55cdd4791ae555934f1d44b563"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
